{
  "hash": "e80092ab1f546ba5e3b1d87c48e95585",
  "result": {
    "markdown": "---\ntitle: \"(tidy)Modeling Workshop\"\nauthor: \"Max Kuhn\"\ntitle-slide-attributes:\n  data-background-image: images/hex_wall.png\n  data-background-size: contain\n  data-background-opacity: \"0.07\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://topepo.github.io/2023_AstraZeneca>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html    \n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n## About me\n\n - Becton-Dickinson (6y): molecular diagnostics for infectious diseases, non-clinical and clinical\n - Pfizer (12y): nonclinical, Med chem, Comp {bio,chem} support\n - <span style=\"color:LightGray;\"><strike>RStudio</strike></span> posit PBC (>= 2016): modeling packages\n \nSelected R packages: [`caret`](https://topepo.github.io/caret/), [`C50`](https://topepo.github.io/C5.0/), [`Cubist`](https://topepo.github.io/Cubist/), a lot of [tidymodels](https://github.com/orgs/tidymodels/repositories)\n\n - [_Applied Predictive Modeling_](http://appliedpredictivemodeling.com)\n - [_Feature Engineering and Selection_](https://bookdown.org/max/FES)\n - [_Tidy Models with R_](http://tmwr.org)\n - [_Nonclinical Statistics for Pharmaceutical and Biotechnology Industries_](https://link.springer.com/book/10.1007/978-3-319-23558-5) (ed, auth) \n\n## Modeling in R\n\n* R has always had a rich set of modeling tools that it inherited from S. For example, the formula interface has made it simple to specify potentially complex model structures.   \n\n* _R has cutting-edge models_. Many researchers in various domains use R as their primary computing environment and their work often results in R packages.\n\n* _It is easy to port or link to other applications_. R doesn't try to be everything to everyone.\n\n\n## Modeling in R\nHowever, there is a huge _consistency problem_. For example: \n\n* There are two primary methods for specifying what terms are in a model. Not all models have both. \n* 99% of model functions automatically generate dummy variables. \n* Many package developers don't know much about the language and omit OOP and other core R components.\n\nTwo examples follow... \n\n\n\n\n## Between-Package Inconsistency\n\nThe syntax for computing predicted class probabilities:\n\n::: {.incremental}\n- `MASS` package: `predict(lda_fit)` \n- `stats` package: `predict(glm_fit, type = \"response\")` \n- `gbm` package: `predict(gbm_fit, type = \"response\", n.trees)`\n- `mda` package: `predict(mda_fit, type = \"posterior\")` \n- `rpart` package: `predict(rpart_fit, type = \"prob\")` \n- `RWeka` package: `predict(bagging_fit, type = \"probability\")` \n- `pamr` package: `pamr.predict(pamr_fit, type = \"posterior\")`\n:::\n\n\n## Within-Package Inconsistency: `glmnet` Predictions\n\n\n\n\n \nThe `glmnet` model can be used to fit regularized generalized linear models with a mixture of L<sub>1</sub> and L<sub>2</sub> penalties. \n\nWe'll look at what happens when we get predictions for a regression model (i.e. numeric _Y_) as well as classification models where _Y_ has two or three categorical values. \n\n- The models shown below contain solutions for three regularization values ( $\\lambda$ ). \n\n- The predict method gives the results for all three at once (👍).\n\n## Numeric `glmnet` Predictions\n\nPredicting a numeric outcome for two new data points:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_x\n#>             x1     x2     x3     x4\n#> sample_1 1.649 -0.483 -0.294 -0.815\n#> sample_2 0.656 -0.420  0.880  0.109\n\npredict(reg_mod, newx = new_x)\n#>          s0 s1 s2\n#> sample_1 10 10 10\n#> sample_2 10 10 10\n```\n:::\n\n\nA matrix result and we will assume that the $\\lambda$ values are in the same order as what we gave to the model fit function.\n\n\n\n## `glmnet` predictions formats\n\n. . .\n\n**Numeric model, numeric prediction**\n\n - numeric sample x penalty array\n\n. . .\n\n**Binary model, class prediction**\n\n- _character_ sample x penalty array \n\n. . .\n\n**Binary model, probability prediction**\n\n- _numeric_ sample x penalty array (values are 2nd factor level)\n\n. . .\n\n**Multinomial model, probability prediction**\n\n- _numeric_ class x sample x penalty array\n\n\n## `glmnet` predictions formats\n\n😳\n\nMost people have at least four different scripts for the same model\n\n> _Am I working for `glmnet` or is it is working for me?_\n\nMaybe a structure like this would work better:\n\n\n::: {.cell}\n\n```\n#> # A tibble: 6 × 5\n#>       a     b     c lambda  .row\n#>   <dbl> <dbl> <dbl>  <dbl> <int>\n#> 1 0.373 0.244 0.383 0.01       1\n#> 2 0.327 0.339 0.334 0.01       1\n#> 3 0.389 0.220 0.391 0.001      2\n#> 4 0.326 0.337 0.338 0.001      2\n#> 5 0.390 0.217 0.392 0.0001     3\n#> 6 0.326 0.336 0.338 0.0001     3\n```\n:::\n\n\n\n\n# tidymodels: Our job is to make modeling data with R <span style=\"color:LightGray;\"><strike>suck less</strike></span> better.\n\n# _It's actually pretty good_\n\n# \"Modeling\" includes everything from classical statistical methods to machine learning. \n\n\n\n## The Tidyverse\n\n\nThe [tidyverse](http://www.tidyverse.org/) is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. \n\n\nThe principles of the tidyverse: \n\n1. Reuse existing data structures.\n1. Compose simple functions with the pipe.\n1. Embrace functional programming.\n1. Design for humans.\n\nThis results in more specific conventions around interfaces, function naming, etc. \n\n## The Tidyverse\n\nFor example, we try to use common prefixes for auto-complete:  `tune_grid()`, `tune_bayes()`, ...\n\nThere is also the notion of [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf):\n\n1. Each variable forms a column.\n1. Each observation forms a row.\n1. Each type of observational unit forms a table.\n\nBased on these ideas, we can create modeling packages that have predictable results and are a pleasure to use. \n\n\n## Tidymodels \n\n`tidymodels` is a collection of modeling packages that live in the tidyverse and are designed in the same way. \n\nMy goals for tidymodels are:\n\n1. Encourage empirical validation and good methodology.\n\n1. Smooth out diverse interfaces.\n\n1. Build highly reusable infrastructure.\n\n1. Enable a wider variety of methodologies.\n\n\n# [`tidymodels.org`](https://www.tidymodels.org/)\n\n# _Tidy Modeling with R_ ([`tmwr.org`](https://www.tmwr.org/))\n\n\n\n\n\n## Selected Modeling Packages \n\n\n* [`broom`](https://broom.tidymodels.org/) takes the messy output of built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns them into tidy data frames.\n\n* [`recipes`](https://recipes.tidymodels.org/) is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other tools.\n\n* [`rsample`](https://rsample.tidymodels.org/) has infrastructure for _resampling_ data so that models can be assessed and empirically validated. \n\n* [`parsnip`](https://parsnip.tidymodels.org/) gives us a unified modeling interface.\n\n* [`tune`](https://tune.tidymodels.org/) has functions for grid search and sequential optimization of model parameters. \n\n\n\n\n\n\n## Loading the Meta-Package \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer(quiet = FALSE)\n\ndata(Chicago, package = \"modeldata\")\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nLet's start by predicting the [ridership of the Chicago \"L\" trains](https://bookdown.org/max/FES/chicago-intro.html). \n\nWe have data over 5,698 days between 2001 and 2016 in `Chicago`.\n\nWhat are our predictors? Date, weather data, home game schedules, 14-day lags at other stations. \n:::\n\n::::\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nchicago_rec <- recipe(ridership ~ ., data = Chicago)\n```\n:::\n\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n```\n:::\n\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) \n```\n:::\n\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\")  \n```\n:::\n\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors())  \n```\n:::\n\n\n\n\nOther selectors are:\n\n * `all_nominal()`, `all_numeric()`, and `has_type()`\n \n * `all_predictors()`, `all_outcomes()`, and `has_role()`\n \n * `all_numeric_predictors()` and `all_nominal_predictors()` too\n \n * Standard `dplyr` selectors like `starts_with()` and so on. \n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) \n```\n:::\n\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"7\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(one_of(stations), num_comp = 10) \n```\n:::\n\n\n\n\n\n## What are our _features_?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"7-8\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  # In the embed package:\n  step_umap(one_of(stations), outcome = vars(ridership), num_comp = 10) \n```\n:::\n\n\n\n\n\n## What are our _features_?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"7\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_spline_natural(Harlem, deg_free = 5) \n```\n:::\n\n\n\n\n## What are our _features_? \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"7\"}\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_mutate(temp = (32 * temp - 32) * 5 / 9 ) \n```\n:::\n\n\n<br><br>\n\n***Let's fit a linear regression model!***\n\nWith `parsnip`, we first create an object that specifies the _type_ of model and then the software _engine_ to do the fit. \n\n\n\n## Linear regression specification \n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_mod <- linear_reg() \n\n# Defaults to `lm()`\n```\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n\nThis says \"Let's fit a model with a numeric outcome, and intercept, and slopes for each predictor.\"\n\n* Other model types include `nearest_neighbors()`, `decision_tree()`, `rand_forest()`, `arima_reg()`, and so on.\n\n\nThe `set_engine()` function gives the details on _how_ it should be fit. \n\n:::\n\n::::\n\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-nope.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"keras\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-nope.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"brulee\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-nope.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"spark\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-nope.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n## Let's fit it with...\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"stan\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-nope.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-yes.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nlinear_mod <- \n  linear_reg(penalty = 0.1, mixture = 0.5) %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/geordi-yes.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::::\n\n\n\n\n\n\n\n## A modeling _workflow_ \n\nWe can _optionally_ bundle the recipe and model together into a <span style=\"color:LightGray;\"><strike>pipeline</strike></span> _workflow_:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglmnet_wflow <- \n  workflow() %>% \n  add_model(linear_mod) %>% \n  add_recipe(chicago_rec) # or add_formula() or add_variables()\n```\n:::\n\n\nFitting and prediction are very easy:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglmnet_fit <- fit(glmnet_wflow, data = Chicago)\n\n# Very east to use compared to glmnet::predict():\npredict(glmnet_fit, Chicago %>% slice(1:7))\n#> # A tibble: 7 × 1\n#>   .pred\n#>   <dbl>\n#> 1 13.8 \n#> 2 15.0 \n#> 3 14.7 \n#> 4 14.6 \n#> 5 14.1 \n#> 6  2.36\n#> 7  1.73\n```\n:::\n\n\n\n\n\n\n## Model tuning \n\nWe probably don't have a good idea of what the `penalty` and `mixture` values should be. \n\nWe can _mark them for tuning_ :\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nlinear_mod <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\")\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(linear_mod)\n```\n:::\n\n\nRecipe arguments can also be simultaneously tuned (e.g. `num_comp` in `step_pca()`). \n\nMore on this in the next example... \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Example: Predicting cognitive function\n\n[Craig-Schapiro et al. (2011)](https://dx.plos.org/10.1371/journal.pone.0018850) describe a clinical study of 333 patients (cognitive impairment or healthy). \n\nCSF samples were taken from all subjects. Data collected on each subject included:\n\n- Demographic characteristics such as age and gender\n- Apolipoprotein E genotype\n- Protein measurements of Aβ, Tau, and a phosphorylated version of Tau (pTau)\n- Protein measurements of 124 exploratory biomarkers, and\n- Clinical dementia scores\n\n\n## The data\n\nThere is some class imbalance: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(ad_data, package = \"modeldata\")\ndim(ad_data)\n#> [1] 333 131\ncount(ad_data, Class)\n#> # A tibble: 2 × 2\n#>   Class        n\n#>   <fct>    <int>\n#> 1 Impaired    91\n#> 2 Control    242\n```\n:::\n\n\nWe'll use stratified sampling to split the data to maintain the frequency distribution. \n\n## Data splitting\n\nThe initial training/test split (3:1) and resampling via the bootstrap: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nad_split <- initial_split(ad_data, strata = Class)\nad_train <- training(ad_split)\nad_test  <- testing(ad_split)\nad_boot <- bootstraps(ad_train, times = 50, strata = Class)\n\nad_boot %>% slice(1) %>% pluck(\"splits\") %>% pluck(1) %>% analysis() %>% count(Class)\n#> # A tibble: 2 × 2\n#>   Class        n\n#>   <fct>    <int>\n#> 1 Impaired    68\n#> 2 Control    181\n```\n:::\n\n\nWe'll use the bootstrap to measure performance during tuning. \n\n## Model and recipe\n\nLet's fit a neural network and use a simple recipe that standardizes the predictors. \n\nWe'll tune three model parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnet_mod <- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% \n  set_mode(\"classification\")\n\nnnet_rec <- \n  recipe(Class ~ ., data = ad_train) %>% \n  step_dummy(Genotype) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n```\n:::\n\n\n \n\n## Model tuning via racing\n\nWe'll use a tool called _racing_ to tune a large number of model configurations efficiently. \n\n\n::: {.cell hash='index_cache/revealjs/ad-tune_54850fb1c3d371a00798a4523d6cd5ef'}\n\n```{.r .cell-code}\nlibrary(finetune)\n\nset.seed(8239)\nnnet_tune_res <- \n  nnet_mod %>% \n  tune_race_anova(\n    nnet_rec,\n    resamples = ad_boot,\n    grid = 50,\n    control = control_race(verbose_elim = TRUE, save_pred = TRUE)\n  )\n```\n:::\n\n\nThis only fits a fraction of the possible 2500 possible models via efficient interim analysis. \n\n## Racing process\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/racing-plot-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n## Check predictions\n\nLet's take the model with the largest ROC AUC as best:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(nnet_tune_res, metric = \"roc_auc\")\n#> # A tibble: 1 × 9\n#>   hidden_units penalty epochs .metric .estimator  mean     n std_err .config    \n#>          <int>   <dbl>  <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      \n#> 1            2   0.765    978 roc_auc binary     0.879    50 0.00468 Preprocess…\n\nbest_nnet <- select_best(nnet_tune_res, metric = \"roc_auc\")\nbest_nnet\n#> # A tibble: 1 × 4\n#>   hidden_units penalty epochs .config              \n#>          <int>   <dbl>  <int> <chr>                \n#> 1            2   0.765    978 Preprocessor1_Model19\n\noob_pred <- collect_predictions(nnet_tune_res, parameters = best_nnet)\n```\n:::\n\n\nThe predictions are averages of the out-of-sample predictions, \n\n## Check predictions\n\nSo the model separates the classes but are the probabilities well-calibrated? \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/calib-plots-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nYeah but no. Let's mitigate the issue via post-processing using a few different methods. \n\n## Logistic calibration \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(283)\nresampled_pred <- oob_pred %>% vfold_cv() \n\nresampled_pred %>% \n  cal_validate_logistic(truth = Class) %>% \n  collect_metrics()\n#> # A tibble: 2 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.152    10 0.00163 config \n#> 2 brier_class calibrated   binary     0.120    10 0.00296 config\n```\n:::\n\n\nThe Brier score is a good metric to assess how well the model is calibrated.\n\nA value of zero is best and a really bad model for two classes has a value of `(1 - (1/2))^2 = 0.25`.\n\n\n## Isotonic calibration \n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampled_pred %>% \n  cal_validate_isotonic(truth = Class) %>% \n  collect_metrics()\n#> # A tibble: 2 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.152    10 0.00163 config \n#> 2 brier_class calibrated   binary     0.121    10 0.00321 config\n```\n:::\n\n\n## Beta calibration \n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampled_pred %>% \n  cal_validate_beta(truth = Class) %>% \n  collect_metrics()\n#> # A tibble: 2 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.152    10 0.00163 config \n#> 2 brier_class calibrated   binary     0.122    10 0.00285 config\n```\n:::\n\n\nWe'll try using the logistic model. \n\n## Does it work?\n\n:::: {.columns}\n\n::: {.column width=\"56%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nad_cal <- \n  cal_estimate_logistic(oob_pred, truth = Class)\n\ncalibrated_pred <- \n  oob_pred %>% \n  cal_apply(ad_cal)\n\ncalibrated_pred %>%\n  cal_plot_windowed(truth = Class,\n                    estimate = .pred_Impaired,\n                    step_size = 0.025)\n```\n:::\n\n:::\n\n::: {.column width=\"44%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/recal-plot-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n::::\n\n\n## Exercise!\n\nYou have two options: \n\n - Try adding a feature extraction method (_one_ example being PCA) to the recipe and _also_ optimize its parameters. \n \n... or ... \n \n - Use sub-sampling methods for class imbalances to pre-process the data.\n\nHow do we find the possible recipe steps? \n\n## Next steps\n\nIf this model was best, we would fit the model on the entire training set (via the `last_fit()`) function the measure performance on the test set. \n\nSome other things to do with these data: \n\n* [model explainers](https://www.tmwr.org/explain.html)\n\n* [model stacking](https://www.tmwr.org/ensembles.html)\n\n* [model deployment using vetiver](https://rstudio.github.io/vetiver-r/)\n\n\n## Other extensions\n\n- censored data models (a.k.a survival analysis)\n- case weights\n- conformal inference tools for prediction intervals\n\nIn-process:\n\n- model fairness metrics and modeling techniques\n- causal inference methods\n- a general set of post-processing tools\n\n\n## Thanks\n\nThanks for the invitation to speak today!\n\nThe tidymodels team: **Hanna Frick, Emil Hvitfeldt, and Simon Couch**.\n\nSpecial thanks to the other folks who contributed so much to tidymodels: Davis Vaughan, Edgar Ruiz, Alison Hill, Desirée De Leon, our previous interns, and the tidyverse team.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}