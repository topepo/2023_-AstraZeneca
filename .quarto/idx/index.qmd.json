{"title":"(tidy)Modeling Workshop","markdown":{"yaml":{"title":"(tidy)Modeling Workshop","author":"Max Kuhn","title-slide-attributes":{"data-background-image":"images/hex_wall.png","data-background-size":"contain","data-background-opacity":"0.07"},"format":{"revealjs":{"slide-number":true,"footer":"<https://topepo.github.io/2023_AstraZeneca>","include-before-body":"header.html","include-after-body":"footer-annotations.html","theme":["default","tidymodels.scss"],"width":1280,"height":720}},"knitr":{"opts_chunk":{"echo":true,"collapse":true,"comment":"#>"}}},"headingText":"About me","containsRefs":false,"markdown":"\n\n\n\n```{r startup}\n#| include: false\n#| warning: false\n#| message: false\nlibrary(glmnet)\nlibrary(glue)\n\noptions(digits = 3, width = 80)\n\nknitr::opts_chunk$set(\n    comment = \"#>\",\n    fig.path = \"figures/\",\n    dev = 'svg',\n    dev.args = list(bg = \"transparent\")\n  )\n\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores(logical = TRUE))\n\n```\n\n\n - Becton-Dickinson (6y): molecular diagnostics for infectious diseases, non-clinical and clinical\n - Pfizer (12y): nonclinical, Med chem, Comp {bio,chem} support\n - <span style=\"color:LightGray;\"><strike>RStudio</strike></span> posit PBC (>= 2016): modeling packages\n \nSelected R packages: [`caret`](https://topepo.github.io/caret/), [`C50`](https://topepo.github.io/C5.0/), [`Cubist`](https://topepo.github.io/Cubist/), a lot of [tidymodels](https://github.com/orgs/tidymodels/repositories)\n\n - [_Applied Predictive Modeling_](http://appliedpredictivemodeling.com)\n - [_Feature Engineering and Selection_](https://bookdown.org/max/FES)\n - [_Tidy Models with R_](http://tmwr.org)\n - [_Nonclinical Statistics for Pharmaceutical and Biotechnology Industries_](https://link.springer.com/book/10.1007/978-3-319-23558-5) (ed, auth) \n\n## Modeling in R\n\n* R has always had a rich set of modeling tools that it inherited from S. For example, the formula interface has made it simple to specify potentially complex model structures.   \n\n* _R has cutting-edge models_. Many researchers in various domains use R as their primary computing environment and their work often results in R packages.\n\n* _It is easy to port or link to other applications_. R doesn't try to be everything to everyone.\n\n\n## Modeling in R\nHowever, there is a huge _consistency problem_. For example: \n\n* There are two primary methods for specifying what terms are in a model. Not all models have both. \n* 99% of model functions automatically generate dummy variables. \n* Many package developers don't know much about the language and omit OOP and other core R components.\n\nTwo examples follow... \n\n\n\n\n## Between-Package Inconsistency\n\nThe syntax for computing predicted class probabilities:\n\n::: {.incremental}\n- `MASS` package: `predict(lda_fit)` \n- `stats` package: `predict(glm_fit, type = \"response\")` \n- `gbm` package: `predict(gbm_fit, type = \"response\", n.trees)`\n- `mda` package: `predict(mda_fit, type = \"posterior\")` \n- `rpart` package: `predict(rpart_fit, type = \"prob\")` \n- `RWeka` package: `predict(bagging_fit, type = \"probability\")` \n- `pamr` package: `pamr.predict(pamr_fit, type = \"posterior\")`\n:::\n\n\n## Within-Package Inconsistency: `glmnet` Predictions\n\n```{r glmnet-mod}\n#| include: false\nsim_n <- 300\n\nset.seed(1244)\ndat <- data.frame(\n  two_class = rep(letters[1:2], each = sim_n / 2),\n  three_class = rep(letters[1:3], each = sim_n / 3),\n  numeric = rnorm(sim_n) + 10,\n  x1 = rnorm(sim_n),\n  x2 = rnorm(sim_n),\n  x3 = rnorm(sim_n),\n  x4 = rnorm(sim_n)\n)\n\nx <- as.matrix(dat[,-(1:3)])\nnew_x <- head(x, 2)\nrownames(new_x) <- paste0(\"sample_\", 1:2)\n\npenalties <- c(0.0001, 0.001, 0.01)\nreg_mod <-\n  glmnet(x, y = dat$numeric, lambda = penalties)\n\ntwo_class_mod <-\n  glmnet(\n    x,\n    y = dat$two_class,\n    nlambda = 3,\n    family = \"binomial\", \n    lambda = penalties\n  )\n\nthree_class_mod <-\n  glmnet(\n    x,\n    y = dat$three_class,\n    nlambda = 3,\n    family = \"multinomial\", \n    lambda = penalties\n  )\n\nthree_pred <- predict(three_class_mod, newx = new_x, type = \"response\")\nthree_pred <- apply(three_pred, 3, function(x) data.frame(x)) \nthree_pred <- dplyr::bind_rows(three_pred)\nthree_pred <- dplyr::mutate(three_pred, lambda = rep(three_class_mod$lambda, each = 2))\nthree_pred <- tibble::as_tibble(three_pred)\nthree_pred$.row <- rep(1:3, each = 2)\n```\n \nThe `glmnet` model can be used to fit regularized generalized linear models with a mixture of L<sub>1</sub> and L<sub>2</sub> penalties. \n\nWe'll look at what happens when we get predictions for a regression model (i.e. numeric _Y_) as well as classification models where _Y_ has two or three categorical values. \n\n- The models shown below contain solutions for three regularization values ( $\\lambda$ ). \n\n- The predict method gives the results for all three at once (`r emojifont::emoji('+1')`).\n\n## Numeric `glmnet` Predictions\n\nPredicting a numeric outcome for two new data points:\n\n\n```{r glmnet-reg}\nnew_x\n\npredict(reg_mod, newx = new_x)\n```\n\nA matrix result and we will assume that the $\\lambda$ values are in the same order as what we gave to the model fit function.\n\n\n\n## `glmnet` predictions formats\n\n. . .\n\n**Numeric model, numeric prediction**\n\n - numeric sample x penalty array\n\n. . .\n\n**Binary model, class prediction**\n\n- _character_ sample x penalty array \n\n. . .\n\n**Binary model, probability prediction**\n\n- _numeric_ sample x penalty array (values are 2nd factor level)\n\n. . .\n\n**Multinomial model, probability prediction**\n\n- _numeric_ class x sample x penalty array\n\n\n## `glmnet` predictions formats\n\n`r emojifont::emoji('flushed')`\n\nMost people have at least four different scripts for the same model\n\n> _Am I working for `glmnet` or is it is working for me?_\n\nMaybe a structure like this would work better:\n\n```{r better-glmnet}\n#| echo: false\nthree_pred\n```\n\n\n\n# tidymodels: Our job is to make modeling data with R <span style=\"color:LightGray;\"><strike>suck less</strike></span> better.\n\n# _It's actually pretty good_\n\n# \"Modeling\" includes everything from classical statistical methods to machine learning. \n\n\n\n## The Tidyverse\n\n\nThe [tidyverse](http://www.tidyverse.org/) is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. \n\n\nThe principles of the tidyverse: \n\n1. Reuse existing data structures.\n1. Compose simple functions with the pipe.\n1. Embrace functional programming.\n1. Design for humans.\n\nThis results in more specific conventions around interfaces, function naming, etc. \n\n## The Tidyverse\n\nFor example, we try to use common prefixes for auto-complete:  `tune_grid()`, `tune_bayes()`, ...\n\nThere is also the notion of [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf):\n\n1. Each variable forms a column.\n1. Each observation forms a row.\n1. Each type of observational unit forms a table.\n\nBased on these ideas, we can create modeling packages that have predictable results and are a pleasure to use. \n\n\n## Tidymodels \n\n`tidymodels` is a collection of modeling packages that live in the tidyverse and are designed in the same way. \n\nMy goals for tidymodels are:\n\n1. Encourage empirical validation and good methodology.\n\n1. Smooth out diverse interfaces.\n\n1. Build highly reusable infrastructure.\n\n1. Enable a wider variety of methodologies.\n\n\n# [`tidymodels.org`](https://www.tidymodels.org/)\n\n# _Tidy Modeling with R_ ([`tmwr.org`](https://www.tmwr.org/))\n\n\n\n\n\n## Selected Modeling Packages \n\n\n* [`broom`](https://broom.tidymodels.org/) takes the messy output of built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns them into tidy data frames.\n\n* [`recipes`](https://recipes.tidymodels.org/) is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other tools.\n\n* [`rsample`](https://rsample.tidymodels.org/) has infrastructure for _resampling_ data so that models can be assessed and empirically validated. \n\n* [`parsnip`](https://parsnip.tidymodels.org/) gives us a unified modeling interface.\n\n* [`tune`](https://tune.tidymodels.org/) has functions for grid search and sequential optimization of model parameters. \n\n\n\n\n\n\n## Loading the Meta-Package \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{r detach}\n#| include: false\ndetach(\"package:glmnet\", character.only = TRUE)\ndetach(\"package:Matrix\", character.only = TRUE)\ndetach(\"package:stats\", character.only = TRUE)\n```\n```{r load-tm}\n#| warning: false\nlibrary(tidymodels)\ntidymodels_prefer(quiet = FALSE)\n\ndata(Chicago, package = \"modeldata\")\n```\n:::\n\n::: {.column width=\"50%\"}\nLet's start by predicting the [ridership of the Chicago \"L\" trains](https://bookdown.org/max/FES/chicago-intro.html). \n\nWe have data over 5,698 days between 2001 and 2016 in `Chicago`.\n\nWhat are our predictors? Date, weather data, home game schedules, 14-day lags at other stations. \n:::\n\n::::\n\n\n## What are our _features_? \n\n```{r chicago-recipe-base}\nchicago_rec <- recipe(ridership ~ ., data = Chicago)\n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-date}\n#| code-line-numbers: \"2\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-holiday}\n#| code-line-numbers: \"3\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-rm}\n#| code-line-numbers: \"4\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\")  \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-dummy}\n#| code-line-numbers: \"5\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors())  \n```\n\n\n\nOther selectors are:\n\n * `all_nominal()`, `all_numeric()`, and `has_type()`\n \n * `all_predictors()`, `all_outcomes()`, and `has_role()`\n \n * `all_numeric_predictors()` and `all_nominal_predictors()` too\n \n * Standard `dplyr` selectors like `starts_with()` and so on. \n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-norm}\n#| code-line-numbers: \"6\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-pca}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(one_of(stations), num_comp = 10) \n```\n\n\n\n\n## What are our _features_?\n\n```{r chicago-recipe-umap}\n#| code-line-numbers: \"7-8\"\n#| eval: false\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  # In the embed package:\n  step_umap(one_of(stations), outcome = vars(ridership), num_comp = 10) \n```\n\n\n\n\n## What are our _features_?\n\n```{r chicago-recipe-ns}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_spline_natural(Harlem, deg_free = 5) \n```\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-mutate}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_mutate(temp = (32 * temp - 32) * 5 / 9 ) \n```\n\n<br><br>\n\n***Let's fit a linear regression model!***\n\nWith `parsnip`, we first create an object that specifies the _type_ of model and then the software _engine_ to do the fit. \n\n\n\n## Linear regression specification \n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n```{r parsnip-lm-spec}\nlinear_mod <- linear_reg() \n\n# Defaults to `lm()`\n```\n:::\n\n::: {.column width=\"60%\"}\n\nThis says \"Let's fit a model with a numeric outcome, and intercept, and slopes for each predictor.\"\n\n* Other model types include `nearest_neighbors()`, `decision_tree()`, `rand_forest()`, `arima_reg()`, and so on.\n\n\nThe `set_engine()` function gives the details on _how_ it should be fit. \n\n:::\n\n::::\n\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-lm}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-lm-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-keras}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"keras\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-keras-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-torch}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"brulee\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-torch-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-spark}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"spark\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-spark-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with...\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-stan}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"stan\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-stan-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-glmnet}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"glmnet\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-glmnet-yep}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-yes.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n```{r parsnip-glmnet-param}\n#| code-line-numbers: \"2\"\nlinear_mod <- \n  linear_reg(penalty = 0.1, mixture = 0.5) %>% \n  set_engine(\"glmnet\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-glmnet-param-yep}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-yes.png\")\n```\n:::\n\n\n::::\n\n\n```{r reattach, include = FALSE}\nlibrary(glmnet)\nlibrary(stats)\n```\n\n\n## A modeling _workflow_ \n\nWe can _optionally_ bundle the recipe and model together into a <span style=\"color:LightGray;\"><strike>pipeline</strike></span> _workflow_:\n\n```{r workflow}\nglmnet_wflow <- \n  workflow() %>% \n  add_model(linear_mod) %>% \n  add_recipe(chicago_rec) # or add_formula() or add_variables()\n```\n\nFitting and prediction are very easy:\n\n\n```{r workflow-fit}\nglmnet_fit <- fit(glmnet_wflow, data = Chicago)\n\n# Very east to use compared to glmnet::predict():\npredict(glmnet_fit, Chicago %>% slice(1:7))\n```\n\n\n\n\n\n## Model tuning \n\nWe probably don't have a good idea of what the `penalty` and `mixture` values should be. \n\nWe can _mark them for tuning_ :\n\n```{r tuning}\n#| code-line-numbers: \"2\"\nlinear_mod <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\")\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(linear_mod)\n```\n\nRecipe arguments can also be simultaneously tuned (e.g. `num_comp` in `step_pca()`). \n\nMore on this in the next example... \n\n```{r theme}\n#| echo: false\nthm <- theme_bw() + \n  theme(\n    panel.background = element_rect(fill = \"transparent\", colour = NA), \n    plot.background = element_rect(fill = \"transparent\", colour = NA),\n    legend.position = \"top\",\n    legend.background = element_rect(fill = \"transparent\", colour = NA),\n    legend.key = element_rect(fill = \"transparent\", colour = NA)\n  )\ntheme_set(thm)\n```\n\n\n```{r}\n#| label: more-pkgs\n#| echo: false\nlibrary(finetune)\nlibrary(probably)\nlibrary(embed)\nlibrary(themis)\n```\n\n## Example: Predicting cognitive function\n\n[Craig-Schapiro et al. (2011)](https://dx.plos.org/10.1371/journal.pone.0018850) describe a clinical study of 333 patients (cognitive impairment or healthy). \n\nCSF samples were taken from all subjects. Data collected on each subject included:\n\n- Demographic characteristics such as age and gender\n- Apolipoprotein E genotype\n- Protein measurements of Aβ, Tau, and a phosphorylated version of Tau (pTau)\n- Protein measurements of 124 exploratory biomarkers, and\n- Clinical dementia scores\n\n\n## The data\n\nThere is some class imbalance: \n\n```{r}\n#| label: ad-intro\ndata(ad_data, package = \"modeldata\")\ndim(ad_data)\ncount(ad_data, Class)\n```\n\nWe'll use stratified sampling to split the data to maintain the frequency distribution. \n\n## Data splitting\n\nThe initial training/test split (3:1) and resampling via the bootstrap: \n\n```{r}\n#| label: ad-split\nset.seed(12)\nad_split <- initial_split(ad_data, strata = Class)\nad_train <- training(ad_split)\nad_test  <- testing(ad_split)\nad_boot <- bootstraps(ad_train, times = 50, strata = Class)\n\nad_boot %>% slice(1) %>% pluck(\"splits\") %>% pluck(1) %>% analysis() %>% count(Class)\n```\n\nWe'll use the bootstrap to measure performance during tuning. \n\n## Model and recipe\n\nLet's fit a neural network and use a simple recipe that standardizes the predictors. \n\nWe'll tune three model parameters:\n\n```{r}\n#| label: ad-specs\nnnet_mod <- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% \n  set_mode(\"classification\")\n\nnnet_rec <- \n  recipe(Class ~ ., data = ad_train) %>% \n  step_dummy(Genotype) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n```\n\n \n\n## Model tuning via racing\n\nWe'll use a tool called _racing_ to tune a large number of model configurations efficiently. \n\n```{r}\n#| label: ad-tune\n#| results: hide\n#| cache: true\n\nlibrary(finetune)\n\nset.seed(8239)\nnnet_tune_res <- \n  nnet_mod %>% \n  tune_race_anova(\n    nnet_rec,\n    resamples = ad_boot,\n    grid = 50,\n    control = control_race(verbose_elim = TRUE, save_pred = TRUE)\n  )\n```\n\nThis only fits a fraction of the possible `r nrow(ad_boot) * 50` possible models via efficient interim analysis. \n\n## Racing process\n\n```{r}\n#| label: racing-plot\n#| echo: false\n#| out-width: 50%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\n\nplot_race(nnet_tune_res)\n```\n\n## Check predictions\n\nLet's take the model with the largest ROC AUC as best:\n\n```{r}\n#| label: pred-res\nshow_best(nnet_tune_res, metric = \"roc_auc\")\n\nbest_nnet <- select_best(nnet_tune_res, metric = \"roc_auc\")\nbest_nnet\n\noob_pred <- collect_predictions(nnet_tune_res, parameters = best_nnet)\n```\n\nThe predictions are averages of the out-of-sample predictions, \n\n## Check predictions\n\nSo the model separates the classes but are the probabilities well-calibrated? \n\n```{r}\n#| label: calib-plots\n#| echo: false\n#| out-width: 50%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\nlibrary(probably)\ncal_plot_windowed(oob_pred, truth = Class, estimate = .pred_Impaired, step_size = 0.025)\n```\n\nYeah but no. Let's mitigate the issue via post-processing using a few different methods. \n\n## Logistic calibration \n\n```{r}\n#| label: logistic-cal\nset.seed(283)\nresampled_pred <- oob_pred %>% vfold_cv() \n\nresampled_pred %>% \n  cal_validate_logistic(truth = Class) %>% \n  collect_metrics()\n```\n\nThe Brier score is a good metric to assess how well the model is calibrated.\n\nA value of zero is best and a really bad model for two classes has a value of `(1 - (1/2))^2 = 0.25`.\n\n\n## Isotonic calibration \n\n```{r}\n#| label: isotonic-cal\nresampled_pred %>% \n  cal_validate_isotonic(truth = Class) %>% \n  collect_metrics()\n```\n\n## Beta calibration \n\n```{r}\n#| label: beta-cal\nresampled_pred %>% \n  cal_validate_beta(truth = Class) %>% \n  collect_metrics()\n```\n\nWe'll try using the logistic model. \n\n## Does it work?\n\n:::: {.columns}\n\n::: {.column width=\"56%\"}\n```{r}\n#| label: recal-code\n#| eval: false\nad_cal <- \n  cal_estimate_logistic(oob_pred, truth = Class)\n\ncalibrated_pred <- \n  oob_pred %>% \n  cal_apply(ad_cal)\n\ncalibrated_pred %>%\n  cal_plot_windowed(truth = Class,\n                    estimate = .pred_Impaired,\n                    step_size = 0.025)\n```\n:::\n\n::: {.column width=\"44%\"}\n```{r}\n#| label: recal-plot\n#| echo: false\n#| out-width: 80%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\nad_cal <- \n  cal_estimate_logistic(oob_pred, truth = Class)\n\ncalibrated_pred <- \n  oob_pred %>% \n  cal_apply(ad_cal)\n\ncalibrated_pred %>%\n  cal_plot_windowed(truth = Class,\n                    estimate = .pred_Impaired,\n                    step_size = 0.025)\n```\n:::\n\n::::\n\n\n## Exercise!\n\nYou have two options: \n\n - Try adding a feature extraction method (_one_ example being PCA) to the recipe and _also_ optimize its parameters. \n \n... or ... \n \n - Use sub-sampling methods for class imbalances to pre-process the data.\n\nHow do we find the possible recipe steps? \n\n## Next steps\n\nIf this model was best, we would fit the model on the entire training set (via the `last_fit()`) function the measure performance on the test set. \n\nSome other things to do with these data: \n\n* [model explainers](https://www.tmwr.org/explain.html)\n\n* [model stacking](https://www.tmwr.org/ensembles.html)\n\n* [model deployment using vetiver](https://rstudio.github.io/vetiver-r/)\n\n\n## Other extensions\n\n- censored data models (a.k.a survival analysis)\n- case weights\n- conformal inference tools for prediction intervals\n\nIn-process:\n\n- model fairness metrics and modeling techniques\n- causal inference methods\n- a general set of post-processing tools\n\n\n## Thanks\n\nThanks for the invitation to speak today!\n\nThe tidymodels team: **Hanna Frick, Emil Hvitfeldt, and Simon Couch**.\n\nSpecial thanks to the other folks who contributed so much to tidymodels: Davis Vaughan, Edgar Ruiz, Alison Hill, Desirée De Leon, our previous interns, and the tidyverse team.\n","srcMarkdownNoYaml":"\n\n\n\n```{r startup}\n#| include: false\n#| warning: false\n#| message: false\nlibrary(glmnet)\nlibrary(glue)\n\noptions(digits = 3, width = 80)\n\nknitr::opts_chunk$set(\n    comment = \"#>\",\n    fig.path = \"figures/\",\n    dev = 'svg',\n    dev.args = list(bg = \"transparent\")\n  )\n\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores(logical = TRUE))\n\n```\n\n## About me\n\n - Becton-Dickinson (6y): molecular diagnostics for infectious diseases, non-clinical and clinical\n - Pfizer (12y): nonclinical, Med chem, Comp {bio,chem} support\n - <span style=\"color:LightGray;\"><strike>RStudio</strike></span> posit PBC (>= 2016): modeling packages\n \nSelected R packages: [`caret`](https://topepo.github.io/caret/), [`C50`](https://topepo.github.io/C5.0/), [`Cubist`](https://topepo.github.io/Cubist/), a lot of [tidymodels](https://github.com/orgs/tidymodels/repositories)\n\n - [_Applied Predictive Modeling_](http://appliedpredictivemodeling.com)\n - [_Feature Engineering and Selection_](https://bookdown.org/max/FES)\n - [_Tidy Models with R_](http://tmwr.org)\n - [_Nonclinical Statistics for Pharmaceutical and Biotechnology Industries_](https://link.springer.com/book/10.1007/978-3-319-23558-5) (ed, auth) \n\n## Modeling in R\n\n* R has always had a rich set of modeling tools that it inherited from S. For example, the formula interface has made it simple to specify potentially complex model structures.   \n\n* _R has cutting-edge models_. Many researchers in various domains use R as their primary computing environment and their work often results in R packages.\n\n* _It is easy to port or link to other applications_. R doesn't try to be everything to everyone.\n\n\n## Modeling in R\nHowever, there is a huge _consistency problem_. For example: \n\n* There are two primary methods for specifying what terms are in a model. Not all models have both. \n* 99% of model functions automatically generate dummy variables. \n* Many package developers don't know much about the language and omit OOP and other core R components.\n\nTwo examples follow... \n\n\n\n\n## Between-Package Inconsistency\n\nThe syntax for computing predicted class probabilities:\n\n::: {.incremental}\n- `MASS` package: `predict(lda_fit)` \n- `stats` package: `predict(glm_fit, type = \"response\")` \n- `gbm` package: `predict(gbm_fit, type = \"response\", n.trees)`\n- `mda` package: `predict(mda_fit, type = \"posterior\")` \n- `rpart` package: `predict(rpart_fit, type = \"prob\")` \n- `RWeka` package: `predict(bagging_fit, type = \"probability\")` \n- `pamr` package: `pamr.predict(pamr_fit, type = \"posterior\")`\n:::\n\n\n## Within-Package Inconsistency: `glmnet` Predictions\n\n```{r glmnet-mod}\n#| include: false\nsim_n <- 300\n\nset.seed(1244)\ndat <- data.frame(\n  two_class = rep(letters[1:2], each = sim_n / 2),\n  three_class = rep(letters[1:3], each = sim_n / 3),\n  numeric = rnorm(sim_n) + 10,\n  x1 = rnorm(sim_n),\n  x2 = rnorm(sim_n),\n  x3 = rnorm(sim_n),\n  x4 = rnorm(sim_n)\n)\n\nx <- as.matrix(dat[,-(1:3)])\nnew_x <- head(x, 2)\nrownames(new_x) <- paste0(\"sample_\", 1:2)\n\npenalties <- c(0.0001, 0.001, 0.01)\nreg_mod <-\n  glmnet(x, y = dat$numeric, lambda = penalties)\n\ntwo_class_mod <-\n  glmnet(\n    x,\n    y = dat$two_class,\n    nlambda = 3,\n    family = \"binomial\", \n    lambda = penalties\n  )\n\nthree_class_mod <-\n  glmnet(\n    x,\n    y = dat$three_class,\n    nlambda = 3,\n    family = \"multinomial\", \n    lambda = penalties\n  )\n\nthree_pred <- predict(three_class_mod, newx = new_x, type = \"response\")\nthree_pred <- apply(three_pred, 3, function(x) data.frame(x)) \nthree_pred <- dplyr::bind_rows(three_pred)\nthree_pred <- dplyr::mutate(three_pred, lambda = rep(three_class_mod$lambda, each = 2))\nthree_pred <- tibble::as_tibble(three_pred)\nthree_pred$.row <- rep(1:3, each = 2)\n```\n \nThe `glmnet` model can be used to fit regularized generalized linear models with a mixture of L<sub>1</sub> and L<sub>2</sub> penalties. \n\nWe'll look at what happens when we get predictions for a regression model (i.e. numeric _Y_) as well as classification models where _Y_ has two or three categorical values. \n\n- The models shown below contain solutions for three regularization values ( $\\lambda$ ). \n\n- The predict method gives the results for all three at once (`r emojifont::emoji('+1')`).\n\n## Numeric `glmnet` Predictions\n\nPredicting a numeric outcome for two new data points:\n\n\n```{r glmnet-reg}\nnew_x\n\npredict(reg_mod, newx = new_x)\n```\n\nA matrix result and we will assume that the $\\lambda$ values are in the same order as what we gave to the model fit function.\n\n\n\n## `glmnet` predictions formats\n\n. . .\n\n**Numeric model, numeric prediction**\n\n - numeric sample x penalty array\n\n. . .\n\n**Binary model, class prediction**\n\n- _character_ sample x penalty array \n\n. . .\n\n**Binary model, probability prediction**\n\n- _numeric_ sample x penalty array (values are 2nd factor level)\n\n. . .\n\n**Multinomial model, probability prediction**\n\n- _numeric_ class x sample x penalty array\n\n\n## `glmnet` predictions formats\n\n`r emojifont::emoji('flushed')`\n\nMost people have at least four different scripts for the same model\n\n> _Am I working for `glmnet` or is it is working for me?_\n\nMaybe a structure like this would work better:\n\n```{r better-glmnet}\n#| echo: false\nthree_pred\n```\n\n\n\n# tidymodels: Our job is to make modeling data with R <span style=\"color:LightGray;\"><strike>suck less</strike></span> better.\n\n# _It's actually pretty good_\n\n# \"Modeling\" includes everything from classical statistical methods to machine learning. \n\n\n\n## The Tidyverse\n\n\nThe [tidyverse](http://www.tidyverse.org/) is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. \n\n\nThe principles of the tidyverse: \n\n1. Reuse existing data structures.\n1. Compose simple functions with the pipe.\n1. Embrace functional programming.\n1. Design for humans.\n\nThis results in more specific conventions around interfaces, function naming, etc. \n\n## The Tidyverse\n\nFor example, we try to use common prefixes for auto-complete:  `tune_grid()`, `tune_bayes()`, ...\n\nThere is also the notion of [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf):\n\n1. Each variable forms a column.\n1. Each observation forms a row.\n1. Each type of observational unit forms a table.\n\nBased on these ideas, we can create modeling packages that have predictable results and are a pleasure to use. \n\n\n## Tidymodels \n\n`tidymodels` is a collection of modeling packages that live in the tidyverse and are designed in the same way. \n\nMy goals for tidymodels are:\n\n1. Encourage empirical validation and good methodology.\n\n1. Smooth out diverse interfaces.\n\n1. Build highly reusable infrastructure.\n\n1. Enable a wider variety of methodologies.\n\n\n# [`tidymodels.org`](https://www.tidymodels.org/)\n\n# _Tidy Modeling with R_ ([`tmwr.org`](https://www.tmwr.org/))\n\n\n\n\n\n## Selected Modeling Packages \n\n\n* [`broom`](https://broom.tidymodels.org/) takes the messy output of built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns them into tidy data frames.\n\n* [`recipes`](https://recipes.tidymodels.org/) is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other tools.\n\n* [`rsample`](https://rsample.tidymodels.org/) has infrastructure for _resampling_ data so that models can be assessed and empirically validated. \n\n* [`parsnip`](https://parsnip.tidymodels.org/) gives us a unified modeling interface.\n\n* [`tune`](https://tune.tidymodels.org/) has functions for grid search and sequential optimization of model parameters. \n\n\n\n\n\n\n## Loading the Meta-Package \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{r detach}\n#| include: false\ndetach(\"package:glmnet\", character.only = TRUE)\ndetach(\"package:Matrix\", character.only = TRUE)\ndetach(\"package:stats\", character.only = TRUE)\n```\n```{r load-tm}\n#| warning: false\nlibrary(tidymodels)\ntidymodels_prefer(quiet = FALSE)\n\ndata(Chicago, package = \"modeldata\")\n```\n:::\n\n::: {.column width=\"50%\"}\nLet's start by predicting the [ridership of the Chicago \"L\" trains](https://bookdown.org/max/FES/chicago-intro.html). \n\nWe have data over 5,698 days between 2001 and 2016 in `Chicago`.\n\nWhat are our predictors? Date, weather data, home game schedules, 14-day lags at other stations. \n:::\n\n::::\n\n\n## What are our _features_? \n\n```{r chicago-recipe-base}\nchicago_rec <- recipe(ridership ~ ., data = Chicago)\n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-date}\n#| code-line-numbers: \"2\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-holiday}\n#| code-line-numbers: \"3\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-rm}\n#| code-line-numbers: \"4\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\")  \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-dummy}\n#| code-line-numbers: \"5\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors())  \n```\n\n\n\nOther selectors are:\n\n * `all_nominal()`, `all_numeric()`, and `has_type()`\n \n * `all_predictors()`, `all_outcomes()`, and `has_role()`\n \n * `all_numeric_predictors()` and `all_nominal_predictors()` too\n \n * Standard `dplyr` selectors like `starts_with()` and so on. \n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-norm}\n#| code-line-numbers: \"6\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) \n```\n\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-pca}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(one_of(stations), num_comp = 10) \n```\n\n\n\n\n## What are our _features_?\n\n```{r chicago-recipe-umap}\n#| code-line-numbers: \"7-8\"\n#| eval: false\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  # In the embed package:\n  step_umap(one_of(stations), outcome = vars(ridership), num_comp = 10) \n```\n\n\n\n\n## What are our _features_?\n\n```{r chicago-recipe-ns}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_spline_natural(Harlem, deg_free = 5) \n```\n\n\n\n## What are our _features_? \n\n```{r chicago-recipe-mutate}\n#| code-line-numbers: \"7\"\nchicago_rec <- recipe(ridership ~ ., data = Chicago) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_mutate(temp = (32 * temp - 32) * 5 / 9 ) \n```\n\n<br><br>\n\n***Let's fit a linear regression model!***\n\nWith `parsnip`, we first create an object that specifies the _type_ of model and then the software _engine_ to do the fit. \n\n\n\n## Linear regression specification \n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n```{r parsnip-lm-spec}\nlinear_mod <- linear_reg() \n\n# Defaults to `lm()`\n```\n:::\n\n::: {.column width=\"60%\"}\n\nThis says \"Let's fit a model with a numeric outcome, and intercept, and slopes for each predictor.\"\n\n* Other model types include `nearest_neighbors()`, `decision_tree()`, `rand_forest()`, `arima_reg()`, and so on.\n\n\nThe `set_engine()` function gives the details on _how_ it should be fit. \n\n:::\n\n::::\n\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-lm}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-lm-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-keras}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"keras\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-keras-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-torch}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"brulee\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-torch-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-spark}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"spark\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-spark-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n## Let's fit it with...\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-stan}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"stan\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-stan-nope}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-nope.png\")\n```\n:::\n\n::::\n\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n```{r parsnip-glmnet}\n#| code-line-numbers: \"3\"\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"glmnet\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-glmnet-yep}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-yes.png\")\n```\n:::\n\n::::\n\n## Let's fit it with... \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n```{r parsnip-glmnet-param}\n#| code-line-numbers: \"2\"\nlinear_mod <- \n  linear_reg(penalty = 0.1, mixture = 0.5) %>% \n  set_engine(\"glmnet\")\n\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r parsnip-glmnet-param-yep}\n#| echo: false\n#| out-width: 100%\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/geordi-yes.png\")\n```\n:::\n\n\n::::\n\n\n```{r reattach, include = FALSE}\nlibrary(glmnet)\nlibrary(stats)\n```\n\n\n## A modeling _workflow_ \n\nWe can _optionally_ bundle the recipe and model together into a <span style=\"color:LightGray;\"><strike>pipeline</strike></span> _workflow_:\n\n```{r workflow}\nglmnet_wflow <- \n  workflow() %>% \n  add_model(linear_mod) %>% \n  add_recipe(chicago_rec) # or add_formula() or add_variables()\n```\n\nFitting and prediction are very easy:\n\n\n```{r workflow-fit}\nglmnet_fit <- fit(glmnet_wflow, data = Chicago)\n\n# Very east to use compared to glmnet::predict():\npredict(glmnet_fit, Chicago %>% slice(1:7))\n```\n\n\n\n\n\n## Model tuning \n\nWe probably don't have a good idea of what the `penalty` and `mixture` values should be. \n\nWe can _mark them for tuning_ :\n\n```{r tuning}\n#| code-line-numbers: \"2\"\nlinear_mod <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\")\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(linear_mod)\n```\n\nRecipe arguments can also be simultaneously tuned (e.g. `num_comp` in `step_pca()`). \n\nMore on this in the next example... \n\n```{r theme}\n#| echo: false\nthm <- theme_bw() + \n  theme(\n    panel.background = element_rect(fill = \"transparent\", colour = NA), \n    plot.background = element_rect(fill = \"transparent\", colour = NA),\n    legend.position = \"top\",\n    legend.background = element_rect(fill = \"transparent\", colour = NA),\n    legend.key = element_rect(fill = \"transparent\", colour = NA)\n  )\ntheme_set(thm)\n```\n\n\n```{r}\n#| label: more-pkgs\n#| echo: false\nlibrary(finetune)\nlibrary(probably)\nlibrary(embed)\nlibrary(themis)\n```\n\n## Example: Predicting cognitive function\n\n[Craig-Schapiro et al. (2011)](https://dx.plos.org/10.1371/journal.pone.0018850) describe a clinical study of 333 patients (cognitive impairment or healthy). \n\nCSF samples were taken from all subjects. Data collected on each subject included:\n\n- Demographic characteristics such as age and gender\n- Apolipoprotein E genotype\n- Protein measurements of Aβ, Tau, and a phosphorylated version of Tau (pTau)\n- Protein measurements of 124 exploratory biomarkers, and\n- Clinical dementia scores\n\n\n## The data\n\nThere is some class imbalance: \n\n```{r}\n#| label: ad-intro\ndata(ad_data, package = \"modeldata\")\ndim(ad_data)\ncount(ad_data, Class)\n```\n\nWe'll use stratified sampling to split the data to maintain the frequency distribution. \n\n## Data splitting\n\nThe initial training/test split (3:1) and resampling via the bootstrap: \n\n```{r}\n#| label: ad-split\nset.seed(12)\nad_split <- initial_split(ad_data, strata = Class)\nad_train <- training(ad_split)\nad_test  <- testing(ad_split)\nad_boot <- bootstraps(ad_train, times = 50, strata = Class)\n\nad_boot %>% slice(1) %>% pluck(\"splits\") %>% pluck(1) %>% analysis() %>% count(Class)\n```\n\nWe'll use the bootstrap to measure performance during tuning. \n\n## Model and recipe\n\nLet's fit a neural network and use a simple recipe that standardizes the predictors. \n\nWe'll tune three model parameters:\n\n```{r}\n#| label: ad-specs\nnnet_mod <- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% \n  set_mode(\"classification\")\n\nnnet_rec <- \n  recipe(Class ~ ., data = ad_train) %>% \n  step_dummy(Genotype) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n```\n\n \n\n## Model tuning via racing\n\nWe'll use a tool called _racing_ to tune a large number of model configurations efficiently. \n\n```{r}\n#| label: ad-tune\n#| results: hide\n#| cache: true\n\nlibrary(finetune)\n\nset.seed(8239)\nnnet_tune_res <- \n  nnet_mod %>% \n  tune_race_anova(\n    nnet_rec,\n    resamples = ad_boot,\n    grid = 50,\n    control = control_race(verbose_elim = TRUE, save_pred = TRUE)\n  )\n```\n\nThis only fits a fraction of the possible `r nrow(ad_boot) * 50` possible models via efficient interim analysis. \n\n## Racing process\n\n```{r}\n#| label: racing-plot\n#| echo: false\n#| out-width: 50%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\n\nplot_race(nnet_tune_res)\n```\n\n## Check predictions\n\nLet's take the model with the largest ROC AUC as best:\n\n```{r}\n#| label: pred-res\nshow_best(nnet_tune_res, metric = \"roc_auc\")\n\nbest_nnet <- select_best(nnet_tune_res, metric = \"roc_auc\")\nbest_nnet\n\noob_pred <- collect_predictions(nnet_tune_res, parameters = best_nnet)\n```\n\nThe predictions are averages of the out-of-sample predictions, \n\n## Check predictions\n\nSo the model separates the classes but are the probabilities well-calibrated? \n\n```{r}\n#| label: calib-plots\n#| echo: false\n#| out-width: 50%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\nlibrary(probably)\ncal_plot_windowed(oob_pred, truth = Class, estimate = .pred_Impaired, step_size = 0.025)\n```\n\nYeah but no. Let's mitigate the issue via post-processing using a few different methods. \n\n## Logistic calibration \n\n```{r}\n#| label: logistic-cal\nset.seed(283)\nresampled_pred <- oob_pred %>% vfold_cv() \n\nresampled_pred %>% \n  cal_validate_logistic(truth = Class) %>% \n  collect_metrics()\n```\n\nThe Brier score is a good metric to assess how well the model is calibrated.\n\nA value of zero is best and a really bad model for two classes has a value of `(1 - (1/2))^2 = 0.25`.\n\n\n## Isotonic calibration \n\n```{r}\n#| label: isotonic-cal\nresampled_pred %>% \n  cal_validate_isotonic(truth = Class) %>% \n  collect_metrics()\n```\n\n## Beta calibration \n\n```{r}\n#| label: beta-cal\nresampled_pred %>% \n  cal_validate_beta(truth = Class) %>% \n  collect_metrics()\n```\n\nWe'll try using the logistic model. \n\n## Does it work?\n\n:::: {.columns}\n\n::: {.column width=\"56%\"}\n```{r}\n#| label: recal-code\n#| eval: false\nad_cal <- \n  cal_estimate_logistic(oob_pred, truth = Class)\n\ncalibrated_pred <- \n  oob_pred %>% \n  cal_apply(ad_cal)\n\ncalibrated_pred %>%\n  cal_plot_windowed(truth = Class,\n                    estimate = .pred_Impaired,\n                    step_size = 0.025)\n```\n:::\n\n::: {.column width=\"44%\"}\n```{r}\n#| label: recal-plot\n#| echo: false\n#| out-width: 80%\n#| fig-width: 5\n#| fig-height: 5\n#| fig-align: \"center\"\nad_cal <- \n  cal_estimate_logistic(oob_pred, truth = Class)\n\ncalibrated_pred <- \n  oob_pred %>% \n  cal_apply(ad_cal)\n\ncalibrated_pred %>%\n  cal_plot_windowed(truth = Class,\n                    estimate = .pred_Impaired,\n                    step_size = 0.025)\n```\n:::\n\n::::\n\n\n## Exercise!\n\nYou have two options: \n\n - Try adding a feature extraction method (_one_ example being PCA) to the recipe and _also_ optimize its parameters. \n \n... or ... \n \n - Use sub-sampling methods for class imbalances to pre-process the data.\n\nHow do we find the possible recipe steps? \n\n## Next steps\n\nIf this model was best, we would fit the model on the entire training set (via the `last_fit()`) function the measure performance on the test set. \n\nSome other things to do with these data: \n\n* [model explainers](https://www.tmwr.org/explain.html)\n\n* [model stacking](https://www.tmwr.org/ensembles.html)\n\n* [model deployment using vetiver](https://rstudio.github.io/vetiver-r/)\n\n\n## Other extensions\n\n- censored data models (a.k.a survival analysis)\n- case weights\n- conformal inference tools for prediction intervals\n\nIn-process:\n\n- model fairness metrics and modeling techniques\n- causal inference methods\n- a general set of post-processing tools\n\n\n## Thanks\n\nThanks for the invitation to speak today!\n\nThe tidymodels team: **Hanna Frick, Emil Hvitfeldt, and Simon Couch**.\n\nSpecial thanks to the other folks who contributed so much to tidymodels: Davis Vaughan, Edgar Ruiz, Alison Hill, Desirée De Leon, our previous interns, and the tidyverse team.\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"svg","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","include-before-body":["header.html"],"include-after-body":["footer-annotations.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.313","auto-stretch":true,"title":"(tidy)Modeling Workshop","author":"Max Kuhn","title-slide-attributes":{"data-background-image":"images/hex_wall.png","data-background-size":"contain","data-background-opacity":"0.07"},"knitr":{"opts_chunk":{"echo":true,"collapse":true,"comment":"#>"}},"slideNumber":true,"footer":"<https://topepo.github.io/2023_AstraZeneca>","theme":["default","tidymodels.scss"],"width":1280,"height":720}}},"projectFormats":["html"]}